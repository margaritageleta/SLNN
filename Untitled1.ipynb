{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_show(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import line_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = np.random.normal(size = 35, scale = 0.1)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import line_search\n",
    "#from optinpy.optinpy.linesearch import interp23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ls import linesearch\n",
    "def GM(x, f, g, eps, kmax, precision = 6):\n",
    "    gradient_norm = round(np.linalg.norm(g(x)), precision)\n",
    "    Xk = [[np.NaN, f(x), gradient_norm]]\n",
    "    #==============#\n",
    "    k = 0\n",
    "    almax = 1\n",
    "    while np.linalg.norm(g(x)) > eps and k < kmax:\n",
    "        d = -g(x)\n",
    "        if k != 0:\n",
    "            almax = np.squeeze(2 * (f(x) - f(x_prev)) / (g(x).T @ d))\n",
    "        alpha, iout = linesearch(f, g, x, d, alpham = almax, c1 = 0.01, c2 = 0.45, maxiter = 500, eps = 10e-6)\n",
    "        x_prev = x\n",
    "        x = x + alpha*d\n",
    "        k += 1\n",
    "        #===========#\n",
    "        gradient_norm = np.round(np.linalg.norm(g(x)), precision)\n",
    "        Xk.append([alpha, f(x), gradient_norm])\n",
    "        #===========#\n",
    "        #print(\"[GM] Final:\",x.T)\n",
    "        #print(\"[GM] Iterations:\", k)\n",
    "    data = pd.DataFrame(Xk, columns=[\"alpha\", \"f(x)\", \"||g(x)||\"], dtype=np.float)\n",
    "    return x, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(x, f, g, eps, kmax, precision = 6):\n",
    "    gradient_norm = np.round(np.linalg.norm(g(x)), precision)\n",
    "    Xk = [[np.NaN, f(x), gradient_norm]]\n",
    "\n",
    "    H = I = np.identity(len(g(x)))\n",
    "    k = 0\n",
    "    almax = 1\n",
    "    while np.linalg.norm(g(x)) > eps and k < kmax:\n",
    "        d = -H @ g(x)\n",
    "        if k != 0:\n",
    "            almax = np.squeeze(2 * (f(x) - f(x_prev)) / (g(x).T @ d))\n",
    "            #almax = old_al * ((old_g.T) @ old_d) / (g(x).T @ d)\n",
    "        #alpha, iout = linesearch(f, g, x, d, alpham = almax, c1 = 0.01, c2 = 0.45, maxiter = 50, eps = 10e-6)\n",
    "        if k > 0:\n",
    "            alpha, *_ = line_search(f, g, x, d, old_old_fval=f(x_prev), c1 = 0.01, c2 = 0.45)\n",
    "        else:\n",
    "            alpha, *_ = line_search(f, g, x, d, c1 = 0.01, c2 = 0.45)\n",
    "        if alpha is None:\n",
    "            print(\"Stopped because of alpha (!)\")\n",
    "            return x, pd.DataFrame(Xk, columns=[\"alpha\", \"f(x)\", \"||g(x)||\"], dtype=np.float)\n",
    "        old_g = g(x)\n",
    "        old_d = d\n",
    "        old_al = alpha\n",
    "        x, x_prev = x + alpha*d, x\n",
    "        s = x - x_prev\n",
    "        y = g(x) - g(x_prev)\n",
    "        y = y[None, :]\n",
    "        rho = 1 / ((y).T @ s)\n",
    "        H = (I - rho * s @ y.T) @ H @ (I - rho * y @ (s.T)) + rho * s @ s.T\n",
    "        k += 1\n",
    "        \n",
    "        gradient_norm = np.round(np.linalg.norm(g(x)), precision)\n",
    "        Xk.append([alpha, f(x), gradient_norm])\n",
    "    return x, pd.DataFrame(Xk, columns=[\"alpha\", \"f(x)\", \"||g(x)||\"], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CGM(x, f, g, eps, kmax, iCG, iRC):\n",
    "    d = -g(x)\n",
    "    k = 0\n",
    "    while np.linalg.norm(g(x)) > eps and k < kmax:\n",
    "        alpha, iout = linesearch(f, g, x, d, 50, c1 = 0.01, c2 = 0.45, maxiter = 500, eps = 10e-6)\n",
    "        x, x_prev = x + alpha*d, x\n",
    "        # ============================================================================================================ #  \n",
    "        # CGM variants\n",
    "        if iCG == \"FR\":\n",
    "            beta = (g(x).T @ g(x)) / (g(x_prev).T @ g(x_prev))\n",
    "        elif iCG == \"PR\":\n",
    "            beta = max(0, g(x).T @ (g(x) - g(x_prev)) / (g(x_prev).T @ g(x_prev)))\n",
    "        else:\n",
    "            raise TypeError(\"iCG should be FR (Fletcher-Reeves) or PR (Polak-Ribière)\")\n",
    "        # ============================================================================================================ # \n",
    "        # Restart conditions\n",
    "        if iRC > 0 and nu is None:\n",
    "            raise TypeError(f\"nu is a necessary parameter with iRC equal to {iRC}\")\n",
    "        if (iRC == 1 and k % nu == 0 or\n",
    "                iRC == 2 and g(x).T @ g(x_prev) / np.linalg.norm(g(x))**2 > nu or\n",
    "                k == 0):\n",
    "            d = -g(x)\n",
    "        else:\n",
    "            d = -g(x) + beta*d\n",
    "        # ============================================================================================================ # \n",
    "        k += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(X, w):\n",
    "    \"\"\"Evaluates a SLNN with weigths `w` \"\"\"\n",
    "\n",
    "    return sigmoid((sigmoid(X) @ w.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss(w, X, ytr, p=0):\n",
    "    #return (2 * sigmoid(X) * ((y(X, w) - ytr) * y(X, w) * (1 - y(X, w))) + p*w).sum(axis=0)\n",
    "    return np.squeeze(2 * sigmoid(X.T) @ ((y(X, w) - ytr) * y(X, w) * (1 - y(X, w))) + p*w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_loss(np.full((1, 35), 2), train, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_loss(wt, train, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opty = BFGS(wt, lambda w: loss(w, train, ytr, 30), lambda w: g_loss(w, train, ytr, 30), 1e-6, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_show(opty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_loss(opty, train, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss(opty, train, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.round(np.squeeze(y(train, opty)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.squeeze(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(data):\n",
    "    if len(data) < 10:\n",
    "        return data\n",
    "    data_out = data.head(5)\n",
    "    data_out = data_out.append(data.tail(5))\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize.linesearch import line_search\n",
    "from scipy.optimize.linesearch import line_search_BFGS\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#def nnet(Xtrain, Ytrain, lambd = 0.00, epsilon = 1.0e-06, kmax = 500, BLS_params, optimizer):\n",
    "def nnet(Xtrain, Ytrain, optimizer, epsilon = 1.0e-06, kmax = 500):\n",
    "        ### TRAIN ###\n",
    "        \n",
    "        \n",
    "        \"\"\" init rand weights \"\"\" \n",
    "        ini_weights = np.random.normal(size = 35)[None, :]\n",
    "    \n",
    "        if optimizer == \"GM\": # Steepest descent\n",
    "            \"\"\" train SLNN: \"\"\" \n",
    "            opt_w = GM(ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax)\n",
    "            num_show(opt_w)\n",
    "            print(\"Loss:\", loss(opt_w, Xtrain, Ytrain))\n",
    "            return opt_w\n",
    "\n",
    "        elif optimizer == \"CGM\": # Conjugate Gradient method\n",
    "            opt_w = CGM(ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax, \"FR\", 0)\n",
    "            num_show(opt_w)\n",
    "            print(\"Loss:\", loss(opt_w, Xtrain, Ytrain))\n",
    "\n",
    "        elif optimizer == \"BFGS\": # Quasi-Newton BFGS\n",
    "            opt_w = BFGS(ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax)\n",
    "            #opt_w, min_loss, info = scipy.optimize.fmin_l_bfgs_b(lambda w: loss(w, train, ytr), wt, fprime=(lambda w: g_loss(w, train, ytr)))\n",
    "            num_show(opt_w)\n",
    "            print(\"Loss:\", loss(opt_w, Xtrain, Ytrain))\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid optimizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet:\n",
    "    def __init__(self, Xtrain, Ytrain):\n",
    "        self.Xtrain          = Xtrain\n",
    "        self.ini_weights    = np.random.normal(size = 35)[None, :]            \n",
    "        self.Ytrain        = Ytrain\n",
    "        \n",
    "    def accuracy_train(self, tuned_weights):\n",
    "        delta, num_samples = 0, len(self.Ytrain)\n",
    "        for i in range(num_samples):\n",
    "            if abs(np.round(y(self.Xtrain, tuned_weights)[i],2) - self.Ytrain[i]) < 1e-06: delta += 1\n",
    "        accuracy = (100/num_samples)*delta\n",
    "        print(\"Accuracy train: \", accuracy, \"%\")\n",
    "        return accuracy\n",
    "    \n",
    "    def accuracy_test(self, Xtest, Ytest, tuned_weights):\n",
    "        delta, num_samples = 0, len(Ytest)\n",
    "        for i in range(num_samples):\n",
    "            if abs(np.round(y(Xtest, tuned_weights)[i],2) - Ytest[i]) < 1e-06: delta += 1\n",
    "        accuracy = (100/num_samples)*delta\n",
    "        print(\"Accuracy test: \", accuracy, \"%\")\n",
    "        \n",
    "        if nnet.accuracy_train(self, tuned_weights)*0.25 > accuracy:\n",
    "            print(\"Possibly overfitted model!\")\n",
    "        \n",
    "    def train(self, optimizer, epsilon, kmax):\n",
    "        \"\"\" train SLNN: \"\"\" \n",
    "        if optimizer == \"GM\":\n",
    "            tuned_weights = GM(self.ini_weights, lambda w: loss(w, self.Xtrain, self.Ytrain), lambda w: g_loss(w, self.Xtrain, self.Ytrain), epsilon, kmax)\n",
    "        elif optimizer == \"CGM\":\n",
    "            tuned_weights = CGM(self.ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax, \"FR\", 0)\n",
    "        elif optimizer == \"BFGS\":\n",
    "            tuned_weights = BFGS(self.ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax)\n",
    "        else:\n",
    "            print(\"Invalid optimizer.\")\n",
    "            return self.ini_weights\n",
    "        num_show(tuned_weights)\n",
    "        print(\"Loss:\", loss(tuned_weights, self.Xtrain, self.Ytrain))\n",
    "        #nnet.accuracy_train(self, opt_w)\n",
    "        return tuned_weights\n",
    "\n",
    "    def test(self, Xtest, Ytest, tuned_weights):\n",
    "        nnet.accuracy_test(self, Xtest, Ytest, tuned_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLNN:\n",
    "    def __init__(self, n = 35):\n",
    "        self.weights    = np.zeros((1, 35))\n",
    "        self._trained = False\n",
    "        self.out = None\n",
    "        \n",
    "    def train(self, optimizer, x, y, p = 0, epsilon=10e-6, kmax = 1000):\n",
    "        if optimizer == \"GM\":\n",
    "            self.weights, self.out = GM(self.weights, lambda w: loss(w, x, y, p), lambda w: g_loss(w, x, y, p), epsilon, kmax)\n",
    "        elif optimizer == \"CGM\":\n",
    "            self.weights, self.out =  CGM(self.weights, lambda w: loss(w, x, y, p), lambda w: g_loss(w, x, y, p), epsilon, kmax, \"FR\", 0), None\n",
    "        elif optimizer == \"BFGS\":\n",
    "            self.weights, self.out = BFGS(self.weights, lambda w: loss(w, x, y, p), lambda w: g_loss(w, x, y, p), epsilon, kmax)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimizer.\")\n",
    "        self.x, self.y, self.p = x, y, p\n",
    "        self.optimizer = optimizer\n",
    "        self._trained = True\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return np.round(y(x, self.weights))\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        return 100 * np.sum(self.predict(x) == y) / len(y)\n",
    "    \n",
    "    def summary(self, Xte, yte):\n",
    "        if not self._trained:\n",
    "            print(\"Model is not trained yet\")\n",
    "        else:\n",
    "            print(f\"Single Layer Neural Network (SLNN)\")\n",
    "            print(\"-\"*75)\n",
    "            print(f\"Input size:\\t\\t\\t{len(np.squeeze(self.weights))}\")\n",
    "            print(f\"Output size:\\t\\t\\t1 (binary)\")\n",
    "            print(\"-\"*75)\n",
    "            print(f\"Train data:\\t\\t\\t{len(self.x)} observations\")\n",
    "            print(f\"Chosen optimization routine:\\t{self.optimizer}\")\n",
    "            print(f\"Regularization parameter:\\t{self.p}\")\n",
    "            print(f\"Loss:\\t\\t\\t\\t{loss(self.weights, self.x, self.y, self.p)[0]}\")\n",
    "            print(f\"Training accuracy:\\t\\t{self.accuracy(self.x, self.y)}%\")\n",
    "            print(f\"Test accuracy:\\t\\t\\t{self.accuracy(Xte, yte)}%\")\n",
    "            print(\"-\"*75)\n",
    "            print(\"Gradient: \\n\", g_loss(self.weights, self.x, self.y, self.p))\n",
    "            print(\"-\"*75)\n",
    "            num_show(self.weights)\n",
    "            return print_result(self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/3omni/Documents/GitHub/SLNN/main.py:92: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. / (1. + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Layer Neural Network (SLNN)\n",
      "---------------------------------------------------------------------------\n",
      "Input size:\t\t\t35\n",
      "Output size:\t\t\t1 (binary)\n",
      "---------------------------------------------------------------------------\n",
      "Train data:\t\t\t500 observations\n",
      "Chosen optimization routine:\tGM\n",
      "Regularization parameter:\t0\n",
      "Loss:\t\t\t\t1.8216667708018856e-05\n",
      "Training accuracy:\t\t100.0%\n",
      "Test accuracy:\t\t\t100.0%\n",
      "---------------------------------------------------------------------------\n",
      "Gradient: \n",
      " [ 1.39508877e-06  6.86067063e-07  1.25624980e-06  9.27444777e-09\n",
      "  1.24933761e-06  1.02400665e-06  1.45817790e-06  2.27446974e-06\n",
      " -2.02614002e-07  9.31133081e-07  2.83716521e-08  1.18237844e-06\n",
      "  2.22951532e-06 -1.55850183e-06  7.33653553e-07  3.12194833e-07\n",
      "  3.87212822e-06  1.61579652e-06  2.89661978e-08  1.05201981e-06\n",
      "  5.01824374e-07  1.10124825e-06  3.82445203e-07 -8.19933923e-07\n",
      "  5.95582419e-08  3.78435258e-06  1.08067482e-06  2.50751307e-06\n",
      " -3.45722870e-07  1.07753526e-06  1.09897810e-06  3.78349079e-06\n",
      "  3.42813547e-06  1.57444321e-06  1.03685962e-06]\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>f(x)</th>\n",
       "      <th>||g(x)||</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>40.188919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.796008</td>\n",
       "      <td>24.173601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093438</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.053878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.622513</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136.503566</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1910.660877</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2484.514941</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1929.837670</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2532.521113</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1949.029427</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           alpha        f(x)   ||g(x)||\n",
       "0            NaN  125.000000  40.188919\n",
       "1       1.000000   15.796008  24.173601\n",
       "2       0.093438    0.008158   0.053878\n",
       "3      10.622513    0.000228   0.000674\n",
       "4     136.503566    0.000210   0.000291\n",
       "295  1910.660877    0.000019   0.000010\n",
       "296  2484.514941    0.000019   0.000012\n",
       "297  1929.837670    0.000018   0.000010\n",
       "298  2532.521113    0.000018   0.000011\n",
       "299  1949.029427    0.000018   0.000010"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [4], 0.5, 0.1)\n",
    "net = SLNN()\n",
    "net.train(\"GM\", train, ytr)\n",
    "net.summary(test, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnetGM = nnet(train, ytr)\n",
    "tuning = nnetGM.train(\"GM\", 1.0e-06, kmax = 500)\n",
    "nnetGM.test(test, yte, tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnetCGM = nnet(train, ytr)\n",
    "tuning = nnetGM.train(\"CGM\", 1.0e-06, kmax = 500)\n",
    "nnetCGM.test(test, yte, tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnetBFGS = nnet(train, ytr)\n",
    "tuning = nnetGM.train(\"BFGS\", 1.0e-06, kmax = 500)\n",
    "nnetBFGS.test(test, yte, tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "#nnet(train, ytr, \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnet(train, ytr, \"CGM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1,3,5,7,9], 0.5, 0)\n",
    "nnet(train, ytr, \"GM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [0,2,4,6,8], 0.5, 0)\n",
    "nnet(train, ytr, \"GM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [0,2,4,6,8], 0.5, 0)\n",
    "nnet(train, ytr, \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnet(train, ytr, \"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MATLAB test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mXtr = np.array([\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]).T\n",
    "mytr = np.array([[0, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss(np.array([[2, 2, 2]]), mXtr, mytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
