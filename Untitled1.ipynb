{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize.linesearch import line_search\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = np.random.normal(size = 35)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = minimize(lambda w: loss(w, train, ytr), wt, method = 'BFGS')\n",
    "opt_w = opt.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show(opt_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize.linesearch import line_search_BFGS, scalar_search_wolfe2\n",
    "from optinpy.optinpy.linesearch import interp23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GM(x, f, g, eps, kmax):\n",
    "    k = 0\n",
    "    while np.linalg.norm(g(x)) > eps and k < kmax:\n",
    "        d = -g(x)\n",
    "        alpha, *_ = line_search(f, g, x, d)\n",
    "        x = x + alpha*d\n",
    "        k += 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp23(lambda w: np.squeeze(loss(w, train, ytr)), np.squeeze(wt), -g_loss(wt, train, ytr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(x, f, g, eps, kmax):\n",
    "    H = I = np.identity(len(g(x)))\n",
    "    k = 0\n",
    "    old_fk = f(x)\n",
    "    while np.linalg.norm(g(x)) > eps and k < kmax:\n",
    "        d = -H @ g(x)\n",
    "        #alpha, *_ = \n",
    "        #alpha = interp23(lambda x: -np.squeeze(f(x)), np.squeeze(x), d, alpha = 1, alpha_min = 0.1, rho = 0.9,c = 1e-4, max_iter = 10)['alpha']\n",
    "        #alpha, *_ = scalar_search_wolfe2(lambda s: f(x + s), lambda s: g(x + s))\n",
    "        alpha, *_ = line_search(f, g, x, d, c1 = 0.01, c2 = 0.45)\n",
    "        #alpha, *_ = line_search_BFGS(f, lambda x: g(x).T, x, d.T, c1 = 0.01, old_fval=1)\n",
    "        #print(alpha)\n",
    "        old_fk = f(x)\n",
    "        if alpha is None:\n",
    "            print(\"α is not found ⚠️\")\n",
    "            return x\n",
    "        x, x_prev = x + alpha*d, x\n",
    "        s = x - x_prev\n",
    "        y = g(x) - g(x_prev)\n",
    "        y = y[None, :]\n",
    "        rho = 1 / ((y).T @ s)\n",
    "        H = (I - rho * s @ y.T) @ H @ (I - rho * y @ (s.T)) + rho * s @ s.T\n",
    "        k += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CGM(x, f, g, eps, kmax, iCG, iRC):\n",
    "    d = -g(x)\n",
    "    k = 0\n",
    "    while np.linalg.norm(g(x)) > eps and k < kmax:\n",
    "        alpha, *_ = line_search(f, g, x, d, c1 = 0.01, c2 = 0.45)\n",
    "        x, x_prev = x + alpha*d, x\n",
    "        # ============================================================================================================ #  \n",
    "        # CGM variants\n",
    "        if iCG == \"FR\":\n",
    "            beta = (g(x).T @ g(x)) / (g(x_prev).T @ g(x_prev))\n",
    "        elif iCG == \"PR\":\n",
    "            beta = max(0, g(x).T @ (g(x) - g(x_prev)) / (g(x_prev).T @ g(x_prev)))\n",
    "        else:\n",
    "            raise TypeError(\"iCG should be FR (Fletcher-Reeves) or PR (Polak-Ribière)\")\n",
    "        # ============================================================================================================ # \n",
    "        # Restart conditions\n",
    "        if iRC > 0 and nu is None:\n",
    "            raise TypeError(f\"nu is a necessary parameter with iRC equal to {iRC}\")\n",
    "        if (iRC == 1 and k % nu == 0 or\n",
    "                iRC == 2 and g(x).T @ g(x_prev) / np.linalg.norm(g(x))**2 > nu or\n",
    "                k == 0):\n",
    "            d = -g(x)\n",
    "        else:\n",
    "            d = -g(x) + beta*d\n",
    "        # ============================================================================================================ # \n",
    "        k += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opty = BFGS(wt, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), 10e-6, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show(opty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_loss(opty, train, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(opty, train, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(y(train, opty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize.linesearch import line_search\n",
    "from scipy.optimize.linesearch import line_search_BFGS\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#def nnet(Xtrain, Ytrain, lambd = 0.00, epsilon = 1.0e-06, kmax = 500, BLS_params, optimizer):\n",
    "def nnet(Xtrain, Ytrain, optimizer, epsilon = 1.0e-06, kmax = 500):\n",
    "        ### TRAIN ###\n",
    "        \n",
    "        \n",
    "        \"\"\" init rand weights \"\"\" \n",
    "        ini_weights = np.random.normal(size = 35)[None, :]\n",
    "    \n",
    "        if optimizer == \"GM\": # Steepest descent\n",
    "            \"\"\" train SLNN: \"\"\" \n",
    "            opt_w = GM(ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax)\n",
    "            num_show(opt_w)\n",
    "            print(\"Loss:\", loss(opt_w, Xtrain, Ytrain))\n",
    "            return opt_w\n",
    "\n",
    "        elif optimizer == \"CGM\": # Conjugate Gradient method\n",
    "            opt_w = CGM(ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax, \"FR\", 0)\n",
    "            num_show(opt_w)\n",
    "            print(\"Loss:\", loss(opt_w, Xtrain, Ytrain))\n",
    "\n",
    "        elif optimizer == \"BFGS\": # Quasi-Newton BFGS\n",
    "            opt_w = BFGS(ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax)\n",
    "            #opt_w, min_loss, info = scipy.optimize.fmin_l_bfgs_b(lambda w: loss(w, train, ytr), wt, fprime=(lambda w: g_loss(w, train, ytr)))\n",
    "            num_show(opt_w)\n",
    "            print(\"Loss:\", loss(opt_w, Xtrain, Ytrain))\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid optimizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet:\n",
    "    def __init__(self, Xtrain, Ytrain):\n",
    "        self.Xtrain          = Xtrain\n",
    "        self.ini_weights    = np.random.normal(size = 35)[None, :]            \n",
    "        self.Ytrain        = Ytrain\n",
    "        \n",
    "    def accuracy_train(self, tuned_weights):\n",
    "        delta, num_samples = 0, len(self.Ytrain)\n",
    "        for i in range(num_samples):\n",
    "            if abs(np.round(y(self.Xtrain, tuned_weights)[i],2) - self.Ytrain[i]) < 1e-06: delta += 1\n",
    "        accuracy = (100/num_samples)*delta\n",
    "        print(\"Accuracy train: \", accuracy, \"%\")\n",
    "        return accuracy\n",
    "    \n",
    "    def accuracy_test(self, Xtest, Ytest, tuned_weights):\n",
    "        delta, num_samples = 0, len(Ytest)\n",
    "        for i in range(num_samples):\n",
    "            if abs(np.round(y(Xtest, tuned_weights)[i],2) - Ytest[i]) < 1e-06: delta += 1\n",
    "        accuracy = (100/num_samples)*delta\n",
    "        print(\"Accuracy test: \", accuracy, \"%\")\n",
    "        \n",
    "        if nnet.accuracy_train(self, tuned_weights)*0.25 > accuracy:\n",
    "            print(\"Possibly overfitted model!\")\n",
    "        \n",
    "    def train(self, optimizer, epsilon, kmax):\n",
    "        \"\"\" train SLNN: \"\"\" \n",
    "        if optimizer == \"GM\":\n",
    "            tuned_weights = GM(self.ini_weights, lambda w: loss(w, self.Xtrain, self.Ytrain), lambda w: g_loss(w, self.Xtrain, self.Ytrain), epsilon, kmax)\n",
    "        elif optimizer == \"CGM\":\n",
    "            tuned_weights = CGM(self.ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax, \"FR\", 0)\n",
    "        elif optimizer == \"BFGS\":\n",
    "            tuned_weights = BFGS(self.ini_weights, lambda w: loss(w, train, ytr), lambda w: g_loss(w, train, ytr), epsilon, kmax)\n",
    "        else:\n",
    "            print(\"Invalid optimizer.\")\n",
    "            return self.ini_weights\n",
    "        num_show(tuned_weights)\n",
    "        print(\"Loss:\", loss(tuned_weights, self.Xtrain, self.Ytrain))\n",
    "        #nnet.accuracy_train(self, opt_w)\n",
    "        return tuned_weights\n",
    "\n",
    "    def test(self, Xtest, Ytest, tuned_weights):\n",
    "        nnet.accuracy_test(self, Xtest, Ytest, tuned_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLNN:\n",
    "    def __init__(self, n = 35):\n",
    "        self.weights    = np.random.normal(size = n)[None, :]\n",
    "        self._trained = False\n",
    "        \n",
    "    def train(self, optimizer, x, y, p = 0, epsilon=10e-6, kmax = 500):\n",
    "        if optimizer == \"GM\":\n",
    "            self.weights =   GM(self.weights, lambda w: loss(w, x, y, p), lambda w: g_loss(w, x, y, p), epsilon, kmax)\n",
    "        elif optimizer == \"CGM\":\n",
    "            self.weights =  CGM(self.weights, lambda w: loss(w, x, y, p), lambda w: g_loss(w, x, y, p), epsilon, kmax, \"FR\", 0)\n",
    "        elif optimizer == \"BFGS\":\n",
    "            self.weights = BFGS(self.weights, lambda w: loss(w, x, y, p), lambda w: g_loss(w, x, y, p), epsilon, kmax)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimizer.\")\n",
    "        self.x, self.y, self.p = x, y, p\n",
    "        self.optimizer = optimizer\n",
    "        self._trained = True\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return np.round(y(x, self.weights))\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        return 100 * np.sum(self.predict(x) == y) / len(y)\n",
    "    \n",
    "    def summary(self, Xte, yte):\n",
    "        if not self._trained:\n",
    "            print(\"Model is not trained yet\")\n",
    "        else:\n",
    "            print(f\"SLNN of {len(np.squeeze(self.weights))} neurons\")\n",
    "            print(f\"Train data: {len(self.x)} observations\")\n",
    "            print(f\"Chosen optimization routine: {self.optimizer}\")\n",
    "            print(f\"Regularization parameter: {self.p}\")\n",
    "            print(f\"Loss: {loss(self.weights, self.x, self.y, self.p):.2f}\")\n",
    "            print(f\"Training accuracy: {self.accuracy(self.x, self.y)}%\")\n",
    "            print(f\"Test accuracy: {self.accuracy(Xte, yte)}%\")\n",
    "            print(\"Gradient: \\n\", g_loss(self.weights, self.x, self.y, self.p))\n",
    "            num_show(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "slnnGM = SLNN()\n",
    "slnnGM.train(\"GM\", train, ytr)\n",
    "slnnGM.summary(test, yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnetGM = nnet(train, ytr)\n",
    "tuning = nnetGM.train(\"GM\", 1.0e-06, kmax = 500)\n",
    "nnetGM.test(test, yte, tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnetCGM = nnet(train, ytr)\n",
    "tuning = nnetGM.train(\"CGM\", 1.0e-06, kmax = 500)\n",
    "nnetCGM.test(test, yte, tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnetBFGS = nnet(train, ytr)\n",
    "tuning = nnetGM.train(\"BFGS\", 1.0e-06, kmax = 500)\n",
    "nnetBFGS.test(test, yte, tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "#nnet(train, ytr, \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnet(train, ytr, \"CGM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1,3,5,7,9], 0.5, 0)\n",
    "nnet(train, ytr, \"GM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [0,2,4,6,8], 0.5, 0)\n",
    "nnet(train, ytr, \"GM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [0,2,4,6,8], 0.5, 0)\n",
    "nnet(train, ytr, \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, ytr, yte = gen_data(123456, 500, [1], 0.5, 0)\n",
    "nnet(train, ytr, \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
